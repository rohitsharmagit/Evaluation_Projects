{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the csv file.\n",
    "\n",
    "df0=pd.read_csv('loan_prediction.csv')\n",
    "df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From the above statistics\n",
    "1. Most of the types are object which need to transform the columns with type as object using label encoder.\n",
    "2. Also Loan_ID is acting as just an identifier so it can be remove from the dataset to improvise modelling.\n",
    "3. In the dataset there are values as 3+ for Dependents which needs to be replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Loan_ID column.\n",
    "\n",
    "df0.drop(['Loan_ID'], axis=1, inplace=True)\n",
    "df0.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the dataset there are values as 3+ for Dependents.\n",
    "# Replacing the value 3+ with 3\n",
    "\n",
    "df0['Dependents'] =df0['Dependents'].replace(to_replace =\"3+\",value =\"3\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking which all columns have null values\n",
    "df0.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the nan or nulls\n",
    "\n",
    "df0['Credit_History'].fillna(df0['Credit_History'].mean(), inplace=True)\n",
    "df0['Self_Employed'].fillna('No', inplace=True)\n",
    "df0['Dependents'].fillna('2', inplace=True)\n",
    "df0['LoanAmount'].fillna(df0['LoanAmount'].mean(), inplace=True)\n",
    "df0['Loan_Amount_Term'].fillna(df0['Loan_Amount_Term'].mean(), inplace=True)\n",
    "df0['Gender'].fillna('Male', inplace=True)\n",
    "df0['Married'].fillna('Yes', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking which all columns have null values\n",
    "df0.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding columns as part of transformation.\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "le= LabelEncoder()\n",
    "\n",
    "#df = le.fit_transform(df0)\n",
    "df0['Dependents'] = le.fit_transform(df0['Dependents'])\n",
    "df0['Gender'] = le.fit_transform(df0['Gender'])\n",
    "df0['Married'] = le.fit_transform(df0['Married'])\n",
    "df0['Education'] = le.fit_transform(df0['Education'])\n",
    "df0['Self_Employed'] = le.fit_transform(df0['Self_Employed'])\n",
    "\n",
    "df0['Property_Area'] = le.fit_transform(df0['Property_Area'])\n",
    "df0['Loan_Status'] = le.fit_transform(df0['Loan_Status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the zscore in order to normalize the data.\n",
    "\n",
    "from scipy.stats import zscore\n",
    "z= np.abs(zscore(df0))\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying for the presence of zscore value of data with threshold of more than 3 std score.\n",
    "\n",
    "threshold=3\n",
    "print(np.where(z>3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the outliers having zscore value of more than 3.\n",
    "data=df0[(z<3).all(axis=1)]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the relation between the ApplicantIncome & Loan Status\n",
    "\n",
    "#plt.bar(data['Credit_History'], data['Loan_Status'], color='g')\n",
    "plt.bar(data['Loan_Status'], data['Self_Employed'], color='g')\n",
    "\n",
    "plt.ylabel('Self')\n",
    "plt.xlabel('Loan_Status')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10)) \n",
    "sns.heatmap(df0.corr(), annot=True, fmt=\".2f\") \n",
    "plt.suptitle(\"Correlation Map\", fontsize=18)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Status\n",
    "\n",
    "    1. Based on the correlation map the Loan_Status is related to only Credit_History.\n",
    "    2. Also Loan Amount is related to Applicant's income.\n",
    "    3. Self_Employed, Dependents, Applicantincome does not correlate much with Loan_Status which can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Self_Employed','Dependents','ApplicantIncome'],axis=1 ,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for skewed data\n",
    "data.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing the skewness with boxcox1p in order to avoid 0 encountered as negative while transformation.\n",
    "\n",
    "from scipy.special import boxcox1p\n",
    "# 0 -> log transform\n",
    "# .5 -> square root transform\n",
    "\n",
    "\n",
    "data['Education']=boxcox1p(data['Education'],0.5)\n",
    "data['CoapplicantIncome']=boxcox1p(data['CoapplicantIncome'],0.5)\n",
    "data['LoanAmount']=boxcox1p(data['LoanAmount'],0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for skewed data\n",
    "data.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Civics Marks \n",
    "data.hist(column='Credit_History')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(data['Credit_History'],data['Loan_Status'],margins=True,margins_name='Total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting x  by excluding AveragePrice column which is y here for prediction.\n",
    "x=data.drop(['Loan_Status'],axis=1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scale = StandardScaler()\n",
    "#scale = MinMaxScaler()\n",
    "x=scale.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settng Y\n",
    "\n",
    "y=data['Loan_Status']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_acc_score=0\n",
    "for r_state in range(42,101):\n",
    "    train_x,test_x,train_y,test_y=train_test_split(x,y,test_size=.25,random_state=r_state)\n",
    "    lg=LogisticRegression()\n",
    "    lg.fit(train_x,train_y)\n",
    "    pred=lg.predict(test_x)\n",
    "    accuracyScore=accuracy_score(test_y,pred)\n",
    "    #print(\"Accuracy_Score corresponding to r_state: \",r_state,\" is \",accuracyScore)\n",
    "    if(accuracyScore>max_acc_score):\n",
    "        max_acc_score=accuracyScore\n",
    "        final_rstate=r_state\n",
    "        \n",
    "print(\"\\n\\n\")\n",
    "print(\"Max_accuracy_Score corresponding to final_r_state: \",final_rstate,\" is \",max_acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the test x & y values and using the random state from above step which is 81.\n",
    "\n",
    "train_x,test_x,train_y,test_y=train_test_split(x,y,test_size=.25,random_state=68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN=KNeighborsClassifier(n_neighbors=13)\n",
    "SV=SVC(kernel=\"linear\", C=1)\n",
    "LR=LogisticRegression()\n",
    "DT=DecisionTreeClassifier(criterion='entropy',max_depth=4)\n",
    "GNB=GaussianNB()\n",
    "RFC=RandomForestClassifier(n_estimators=100,random_state=100)\n",
    "ADC=AdaBoostClassifier(n_estimators=100,random_state=10)\n",
    "GBC=GradientBoostingClassifier(n_estimators=100,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[]\n",
    "models.append(('KNeighborsClassifier',KNN))\n",
    "models.append(('SVC',SV))\n",
    "models.append(('LogisticRegression',LR))\n",
    "models.append(('DecisionTreeClassifier',DT))\n",
    "models.append(('GaussianNB',GNB))\n",
    "models.append(('RandomForestClassifier',RFC))\n",
    "models.append(('AdaBoostClassifier',ADC))\n",
    "models.append(('GradientBoostingClassifier',GBC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model=[]\n",
    "score=[]\n",
    "cvs=[]\n",
    "rocscore=[]\n",
    "\n",
    "for name,model in models:\n",
    "    print(\"--------------\",name,\"--------------\")\n",
    "    Model.append(name)\n",
    "    model.fit(train_x,train_y)\n",
    "    print(model)\n",
    "    pre=model.predict(test_x)\n",
    "    AS=accuracy_score(test_y,pre)\n",
    "    print(\"Accuracy Score: \", AS)\n",
    "    score.append(AS*100)\n",
    "    sc=cross_val_score(model,x,y,cv=10,scoring='accuracy').mean()\n",
    "    print(\"Cross_Val_Score: \", sc)\n",
    "    cvs.append(sc*100)\n",
    "    cm=confusion_matrix(test_y,pre)\n",
    "    print(cm)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=pd.DataFrame({\"Model\": Model, \"Score\": score, \"Cross Val Score\":cvs})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting best parameters for the models using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#parameters for SVC\n",
    "params_svc={'kernel':('linear','rbf'), 'C':[1,10]}\n",
    "\n",
    "#parameter for KNN\n",
    "params_knn={'n_neighbors':np.arange(5,20)}\n",
    "\n",
    "#parameters for DTC\n",
    "params_dtc={'criterion':('gini', 'entropy'), 'max_depth':(4,6,8,12)}   \n",
    "\n",
    "#parameters for RFC,ADA,GBC\n",
    "params={'n_estimators':[100,500],'random_state':[10,100]}\n",
    "\n",
    "    \n",
    "svc=GridSearchCV(SVC(),params_svc)\n",
    "svc.fit(train_x,train_y)\n",
    "print(\"Best parameters for Support Vector Classification:\",svc.best_params_)\n",
    "\n",
    "knn= GridSearchCV(KNeighborsClassifier(),param_grid=params_knn)\n",
    "knn.fit(train_x,train_y)\n",
    "print(\"Best parameters for KNeighborsClassifier\",knn.best_params_)\n",
    "\n",
    "\n",
    "dtc= GridSearchCV(DecisionTreeClassifier(),param_grid=params_dtc,cv=10,scoring='accuracy')\n",
    "dtc.fit(train_x,train_y)\n",
    "print(\"Best parameters for DecisionTreeClassifier\",dtc.best_params_)\n",
    "\n",
    "ada= GridSearchCV(AdaBoostClassifier(),param_grid=params,scoring='accuracy')\n",
    "ada.fit(train_x,train_y)\n",
    "print(\"Best parameters for AdaBoostClassifier: \",ada.best_params_)\n",
    "\n",
    "gbc= GridSearchCV(GradientBoostingClassifier(),param_grid=params,scoring='accuracy')\n",
    "gbc.fit(train_x,train_y)\n",
    "print(\"Best parameters for GradientBoostingClassifier: \",gbc.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the best possible parameters for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN=KNeighborsClassifier(n_neighbors=15)\n",
    "SV=SVC(kernel=\"linear\", C=1)\n",
    "LR=LogisticRegression()\n",
    "DT=DecisionTreeClassifier(criterion='entropy',max_depth=4)\n",
    "GNB=GaussianNB()\n",
    "ADC=AdaBoostClassifier(n_estimators=100,random_state=10)\n",
    "GBC=GradientBoostingClassifier(n_estimators=100,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[]\n",
    "models.append(('KNeighborsClassifier',KNN))\n",
    "models.append(('SVC',SV))\n",
    "models.append(('LogisticRegression',LR))\n",
    "models.append(('DecisionTreeClassifier',DT))\n",
    "models.append(('GaussianNB',GNB))\n",
    "models.append(('AdaBoostClassifier',ADC))\n",
    "models.append(('GradientBoostingClassifier',GBC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model=[]\n",
    "score=[]\n",
    "cvs=[]\n",
    "rocscore=[]\n",
    "\n",
    "for name,model in models:\n",
    "    print(\"--------------\",name,\"--------------\")\n",
    "    Model.append(name)\n",
    "    model.fit(train_x,train_y)\n",
    "    print(model)\n",
    "    pre=model.predict(test_x)\n",
    "    AS=accuracy_score(test_y,pre)\n",
    "    print(\"Accuracy Score: \", AS)\n",
    "    score.append(AS*100)\n",
    "    sc=cross_val_score(model,x,y,cv=10,scoring='accuracy').mean()\n",
    "    print(\"Cross_Val_Score: \", sc)\n",
    "    cvs.append(sc*100)\n",
    "    cm=confusion_matrix(test_y,pre)\n",
    "    print(cm)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=pd.DataFrame({\"Model\": Model, \"Score\": score, \"Cross Val Score\":cvs})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression is the best fit model here with 88% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the prediction data in a file.\n",
    "\n",
    "predictData=pd.DataFrame(pre)\n",
    "data.to_csv('Loan_Predict.csv')\n",
    "predictData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(LR,\"LR_Loan.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
